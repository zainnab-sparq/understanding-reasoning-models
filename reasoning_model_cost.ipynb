{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443d7166",
   "metadata": {},
   "source": [
    "# Reasoning Models Token Cost Demo\n",
    "\n",
    "This notebook demonstrates OpenAI's reasoning models at different reasoning effort levels and compares token usage (input, reasoning, and output tokens) for each level.\n",
    "\n",
    "**Test Prompt:** A spatial reasoning puzzle about a baseball in a box with a hole being shipped from Houston to New York City."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa3e382",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenAI library (uncomment if needed)\n",
    "# !pip install openai pandas matplotlib\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7ae86",
   "metadata": {},
   "source": [
    "## 2. Set Up OpenAI API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1995ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the OpenAI API key\n",
    "# Option 1: Set environment variable OPENAI_API_KEY\n",
    "# Option 2: Uncomment and add your key directly (not recommended for production)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "client = OpenAI()\n",
    "print(\"OpenAI client configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6864a8",
   "metadata": {},
   "source": [
    "## 3. Define the Test Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae10e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"I live in Houston, Texas and I have a box with a big hole in it and I need to ship a baseball. I put the ball into the box. Tape it up. Put a label on it. Then send it to my friend in New York City, New York. He picks up the box on Union Street. Takes a cab to his out on Wilson Blvd and goes into his kitchen. He then opens the box and pours out the contents. Where is the baseball?\"\"\"\n",
    "\n",
    "print(\"Test Prompt:\")\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5474f",
   "metadata": {},
   "source": [
    "## 4. Test with Different Reasoning Levels\n",
    "\n",
    "We'll test GPT-5.2 with different reasoning effort levels using the Responses API:\n",
    "- **Minimal** reasoning effort (fastest, cheapest)\n",
    "- **Low** reasoning effort\n",
    "- **Medium** reasoning effort (default)\n",
    "- **High** reasoning effort (most thorough, highest token usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5bc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to query GPT-5.2 and extract token usage\n",
    "def query_model(model_name, reasoning_effort=None):\n",
    "    \"\"\"Query a model using the Responses API and return response with token usage.\"\"\"\n",
    "    try:\n",
    "        if reasoning_effort:\n",
    "            response = client.responses.create(\n",
    "                model=model_name,\n",
    "                input=[{\"role\": \"user\", \"content\": test_prompt}],\n",
    "                reasoning={\"effort\": reasoning_effort}\n",
    "            )\n",
    "        else:\n",
    "            # For no reasoning, use \"minimal\" effort\n",
    "            response = client.responses.create(\n",
    "                model=model_name,\n",
    "                input=[{\"role\": \"user\", \"content\": test_prompt}],\n",
    "                reasoning={\"effort\": \"minimal\"}\n",
    "            )\n",
    "        \n",
    "        # Extract token usage from the response\n",
    "        usage = response.usage\n",
    "        \n",
    "        # Get reasoning tokens from output_tokens_details\n",
    "        reasoning_tokens = 0\n",
    "        if hasattr(usage, 'output_tokens_details') and usage.output_tokens_details:\n",
    "            reasoning_tokens = getattr(usage.output_tokens_details, 'reasoning_tokens', 0) or 0\n",
    "        \n",
    "        result = {\n",
    "            \"model\": model_name,\n",
    "            \"reasoning_effort\": reasoning_effort if reasoning_effort else \"minimal\",\n",
    "            \"input_tokens\": usage.input_tokens,\n",
    "            \"reasoning_tokens\": reasoning_tokens,\n",
    "            \"output_tokens\": usage.output_tokens,\n",
    "            \"total_tokens\": usage.total_tokens,\n",
    "            \"answer\": response.output_text\n",
    "        }\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"reasoning_effort\": reasoning_effort if reasoning_effort else \"minimal\",\n",
    "            \"error\": str(e),\n",
    "            \"input_tokens\": 0,\n",
    "            \"reasoning_tokens\": 0,\n",
    "            \"output_tokens\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"answer\": f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "print(\"Helper function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb24c8",
   "metadata": {},
   "source": [
    "### 4.1 GPT-5.2 with Minimal Reasoning Effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cec811",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing GPT-5.2 with minimal reasoning effort...\")\n",
    "result_gpt52_minimal = query_model(\"gpt-5.2\")  # defaults to minimal\n",
    "print(f\"\\nModel: {result_gpt52_minimal['model']}\")\n",
    "print(f\"Reasoning Effort: {result_gpt52_minimal['reasoning_effort']}\")\n",
    "print(f\"Input Tokens: {result_gpt52_minimal['input_tokens']}\")\n",
    "print(f\"Reasoning Tokens: {result_gpt52_minimal['reasoning_tokens']}\")\n",
    "print(f\"Output Tokens: {result_gpt52_minimal['output_tokens']}\")\n",
    "print(f\"Total Tokens: {result_gpt52_minimal['total_tokens']}\")\n",
    "print(f\"\\nAnswer: {result_gpt52_minimal['answer'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d45498",
   "metadata": {},
   "source": [
    "### 4.2 GPT-5.2 with Low Reasoning Effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe717044",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing GPT-5.2 with low reasoning effort...\")\n",
    "result_gpt52_low = query_model(\"gpt-5.2\", reasoning_effort=\"low\")\n",
    "print(f\"\\nModel: {result_gpt52_low['model']}\")\n",
    "print(f\"Reasoning Effort: {result_gpt52_low['reasoning_effort']}\")\n",
    "print(f\"Input Tokens: {result_gpt52_low['input_tokens']}\")\n",
    "print(f\"Reasoning Tokens: {result_gpt52_low['reasoning_tokens']}\")\n",
    "print(f\"Output Tokens: {result_gpt52_low['output_tokens']}\")\n",
    "print(f\"Total Tokens: {result_gpt52_low['total_tokens']}\")\n",
    "print(f\"\\nAnswer: {result_gpt52_low['answer'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565fdcd4",
   "metadata": {},
   "source": [
    "### 4.3 GPT-5.2 with Medium Reasoning Effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f6c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing GPT-5.2 with medium reasoning effort...\")\n",
    "result_gpt52_medium = query_model(\"gpt-5.2\", reasoning_effort=\"medium\")\n",
    "print(f\"\\nModel: {result_gpt52_medium['model']}\")\n",
    "print(f\"Reasoning Effort: {result_gpt52_medium['reasoning_effort']}\")\n",
    "print(f\"Input Tokens: {result_gpt52_medium['input_tokens']}\")\n",
    "print(f\"Reasoning Tokens: {result_gpt52_medium['reasoning_tokens']}\")\n",
    "print(f\"Output Tokens: {result_gpt52_medium['output_tokens']}\")\n",
    "print(f\"Total Tokens: {result_gpt52_medium['total_tokens']}\")\n",
    "print(f\"\\nAnswer: {result_gpt52_medium['answer'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb338cc",
   "metadata": {},
   "source": [
    "### 4.4 GPT-5.2 with High Reasoning Effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e0192",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing GPT-5.2 with high reasoning effort...\")\n",
    "result_gpt52_high = query_model(\"gpt-5.2\", reasoning_effort=\"high\")\n",
    "print(f\"\\nModel: {result_gpt52_high['model']}\")\n",
    "print(f\"Reasoning Effort: {result_gpt52_high['reasoning_effort']}\")\n",
    "print(f\"Input Tokens: {result_gpt52_high['input_tokens']}\")\n",
    "print(f\"Reasoning Tokens: {result_gpt52_high['reasoning_tokens']}\")\n",
    "print(f\"Output Tokens: {result_gpt52_high['output_tokens']}\")\n",
    "print(f\"Total Tokens: {result_gpt52_high['total_tokens']}\")\n",
    "print(f\"\\nAnswer: {result_gpt52_high['answer'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f75560",
   "metadata": {},
   "source": [
    "## 5. Compare Token Usage Across All Reasoning Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d33f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results into a dataframe\n",
    "results = [\n",
    "    result_gpt52_minimal,\n",
    "    result_gpt52_low,\n",
    "    result_gpt52_medium,\n",
    "    result_gpt52_high\n",
    "]\n",
    "\n",
    "# Create comparison dataframe\n",
    "df = pd.DataFrame(results)\n",
    "df_display = df[['model', 'reasoning_effort', 'input_tokens', 'reasoning_tokens', 'output_tokens', 'total_tokens']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOKEN USAGE COMPARISON - GPT-5.2 ACROSS REASONING LEVELS\")\n",
    "print(\"=\"*80)\n",
    "print(df_display.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a4f01",
   "metadata": {},
   "source": [
    "## 6. Visualize Token Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad8306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for plotting\n",
    "effort_labels = ['Minimal', 'Low', 'Medium', 'High']\n",
    "df['label'] = effort_labels\n",
    "\n",
    "# Create stacked bar chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Stacked bar chart showing token breakdown\n",
    "x_pos = range(len(df))\n",
    "ax1.bar(x_pos, df['input_tokens'], label='Input Tokens', color='#4CAF50')\n",
    "ax1.bar(x_pos, df['reasoning_tokens'], bottom=df['input_tokens'], \n",
    "        label='Reasoning Tokens', color='#FF9800')\n",
    "ax1.bar(x_pos, df['output_tokens'], \n",
    "        bottom=df['input_tokens'] + df['reasoning_tokens'],\n",
    "        label='Output Tokens', color='#2196F3')\n",
    "\n",
    "ax1.set_xlabel('Reasoning Effort Level', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Token Count', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('GPT-5.2 Token Usage Breakdown by Reasoning Level', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(effort_labels)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Total tokens comparison\n",
    "colors = ['#4CAF50', '#8BC34A', '#FF9800', '#FF5722']\n",
    "ax2.bar(x_pos, df['total_tokens'], color=colors)\n",
    "ax2.set_xlabel('Reasoning Effort Level', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Total Tokens', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('GPT-5.2 Total Token Usage by Reasoning Level', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(effort_labels)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(df['total_tokens']):\n",
    "    ax2.text(i, v + max(df['total_tokens']) * 0.02, str(v), \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db289bd9",
   "metadata": {},
   "source": [
    "## 7. View Full Answers from Each Reasoning Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431666ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULT #{i}: GPT-5.2 ({result['reasoning_effort']})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Input Tokens: {result['input_tokens']}\")\n",
    "    print(f\"Reasoning Tokens: {result['reasoning_tokens']}\")\n",
    "    print(f\"Output Tokens: {result['output_tokens']}\")\n",
    "    print(f\"Total Tokens: {result['total_tokens']}\")\n",
    "    print(f\"\\nFull Answer:\\n{result['answer']}\")\n",
    "    print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d635139e",
   "metadata": {},
   "source": [
    "## 8. Key Insights\n",
    "\n",
    "**About the Responses API:**\n",
    "\n",
    "The OpenAI Responses API (`client.responses.create()`) is the recommended way to use GPT-5.2 and other reasoning models. Key parameters:\n",
    "- `reasoning={\"effort\": \"minimal|low|medium|high\"}` - Controls reasoning depth\n",
    "- `input=[{\"role\": \"user\", \"content\": \"...\"}]` - The input messages\n",
    "- Response includes `output_text` for the answer and `usage.output_tokens_details.reasoning_tokens` for reasoning token count\n",
    "\n",
    "**Expected Observations:**\n",
    "\n",
    "1. **Minimal Reasoning** - Fastest response with minimal internal reasoning. May miss nuanced details in complex problems.\n",
    "\n",
    "2. **Low Reasoning** - Light reasoning overhead, good for straightforward questions.\n",
    "\n",
    "3. **Medium Reasoning** (default) - Balanced approach between speed and thoroughness.\n",
    "\n",
    "4. **High Reasoning** - Maximum reasoning effort with highest token usage. Best for complex, multi-step problems.\n",
    "\n",
    "5. **Token Cost Trade-off**: As reasoning effort increases:\n",
    "   - More **reasoning tokens** are generated (internal \"thinking\")\n",
    "   - Answers tend to be more thorough and accurate\n",
    "   - Total token costs increase significantly\n",
    "   - Response times are longer\n",
    "\n",
    "6. **The Baseball Question**: This tests spatial reasoning - the ball likely fell out through the hole during shipping. Higher reasoning levels should be more likely to catch this critical detail and correctly answer that the baseball is \"somewhere between Houston and New York\" rather than \"in the box.\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
